---
title: "Sampling Swire for Coke Heroes"
author: "Ian's ML"
date:  "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    css: custom-styles.css
    theme: null
    highlight: null
    toc: true
    toc_float: false
---


# Set up 

```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman")

# Use pacman to load (and install if necessary) the specific packages you requested
pacman::p_load(dplyr, ggplot2, tidyverse, tidytext, skimr, readr, tidyr, lubridate, stringr, knitr, kableExtra, tidymodels, pROC, xgboost, doParallel, vip, DALEXtra)
#have to manually load 'caret' for some reason

```

## Taking a sample of the whole dataset

```{r}
df <- readRDS("swire_no_nas.rds")  #inject the data and we will sub-sample



```

```{r}

regions_joinme <- read.csv("states_summary.csv")

unique(regions_joinme$REGION)
# "NORTHERN"    "DESERT_SW"   "PRAIRIE"     "CALI_NEVADA"  "MOUNTAIN"    "SOCAL"   "ARIZONA"    "NEWMEXICO"   "NOCAL"    "COLORADO"    "KANSAS" 

str(regions_joinme)
# Perform a left join using the merge() function
df <- merge(df, regions_joinme[, c("MARKET_KEY", "REGION")], by = "MARKET_KEY", all.x = TRUE)
rm(regions_joinme)

```



### Quick imputations 


```{r}
# Update CALORIC_SEGMENT values: 0 if 'DIET/LIGHT', otherwise 1
df$CALORIC_SEGMENT <- ifelse(df$CALORIC_SEGMENT == "DIET/LIGHT", 0, 1)
df$MARKET_KEY <- as.character(df$MARKET_KEY)
df <- df %>%
  mutate(
    MONTH = as.numeric(substr(DATE, 6, 7)),  # Extract the month from YYYY-MM-DD format
    SEASON = case_when(
      MONTH %in% c(12, 01, 02) ~ "WINTER",
      MONTH %in% c(03, 04, 05) ~ "SPRING",
      MONTH %in% c(06, 07, 08) ~ "SUMMER",
      MONTH %in% c(09, 10, 11) ~ "FALL",
      TRUE ~ NA_character_  # This is just in case there are any undefined values
    )
  )




```


```{r}
str(df)
```


## Making a 10% sample of the data to shrink it 

```{r}
# Assuming df is your dataframe
set.seed(123) # Set a random seed for reproducibility
sampled_df <- df[sample(1:nrow(df), 2446143), ]
rm(df)
```

```{r}
df <- sampled_df
rm(sampled_df)
```

```{r}
#skim(df)
```

```{r}
summary(df)
```
### Linear model on sampled data looks the same largely 

```{r}
# Perform a linear regression with UNIT_SALES as the dependent variable
# and PRICE (or your chosen variable) as the independent variable
linear_model <- lm(DOLLAR_SALES ~ UNIT_SALES, data = df)

# Print the summary of the linear model to see the results
summary(linear_model)

```


```{r fig.width=10, fig.height=10}
# Create a scatter plot with the regression line, colored by MANUFACTURER
ggplot(df, aes(x = UNIT_SALES, y = DOLLAR_SALES, color = MANUFACTURER)) +
  geom_point(alpha = 0.5) +  # Adjust alpha to avoid overplotting, if necessary
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band for clarity
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES by MANUFACTURER",
       x = "UNIT SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position if needed

```

## Taking a look at Diet Smash brand..

```{r }

# create a table of total values by brand
brand_summary <- df %>%
  group_by(BRAND) %>%
  summarise(
    total_units_sold = sum(UNIT_SALES),
    total_revenue = sum(DOLLAR_SALES),
    avg_price = total_revenue / total_units_sold,
    total_days_sold = n() # Count the number of rows for each brand
  ) %>%
  arrange(desc(total_revenue)) %>%  # Order by revenue in descending order
  mutate(rank = row_number()) 

summary(brand_summary)

print(brand_summary[brand_summary$BRAND == "DIET SMASH", ])


```

> Out of 288 brands, DIET SMASH slides in at 150th place in terms of total revenue at an above average price $4.17 vs overall $3.27. 


```{r fig.height=10, fig.width=10}

# Filter the dataframe for only 'DIET SMASH'
filtered_df <- df %>% 
  filter(BRAND == "DIET SMASH")

# Create the plot
ggplot(filtered_df, aes(x = UNIT_SALES, y = DOLLAR_SALES)) +
  geom_point(color = "red", alpha = 1) +  # Bright red points with full opacity
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES for DIET SMASH",
       x = "UNIT SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "none")  


```

> DIET SMASH is not a big seller at base line (our sample only contains abourt 1800 observations). There are 2 distinct trend lines, a high flier group and a low flier group. 
The high flier group follows the trend line better, while staying mostly above it. The low flier group underperforms the trend line signficantly.
The high flier group begins to take an outlier distribution abover 100 unit sales, where dollar sales begin to rapidly outpace the trend line.
The low flier group has a much less steep slope, and remains farily tight to about 125 unit sales, but as soon as things hit 150 the dollar sales begin to rise almost verically to meet their peers in the high flier group. Once DIET SMASH hits 150 unit sales or so the dollars start to roll in.


# Sales by Week of the Year

```{r}
filtered_df %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  ggplot(aes(x = WEEK, y = total_sales)) +
  geom_line(color = "black") +  # Blue line connecting points
  labs(title = "Total Sales by Week of the Year",
       x = "Week of the Year",
       y = "Total Unit Sales") +
  theme_minimal()
```

> There are 4 or 5 signficant peaks in sales by week of Diet Smash with a fairly strong clustering between 20 - 35 weeks.

```{r}
library(zoo)
# Calculate total sales for each group of 13 consecutive weeks
sales_by_group <- filtered_df %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 13, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 13)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_group$week_label <- factor(sales_by_group$week_label, levels = sales_by_group$week_label[order(sales_by_group$WEEK)])
ggplot(sales_by_group, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 13-Week Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```
> DIET SMASH has a strong prounced ride up around week 8 and 35, with peak 13 week between week 22 to 34.

```{r }
#find the best 13 weeks for Kiwano sales
# Calculate total sales for each group of 13 consecutive weeks
sales_by_plum <- df %>%
  filter(str_detect(ITEM, "PLUM")) %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 13, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 13)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_plum$week_label <- factor(sales_by_plum$week_label, levels = sales_by_plum$week_label[order(sales_by_plum$WEEK)])
ggplot(sales_by_plum, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 13-Week Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```

> Plum generally trends upward with the strongest sales after week 30, with week 40 to week 52 being the peak 13 week period. 

```{r }
#find the best 13 weeks for plum, ssd, diet, small or one package sales
# Calculate total sales for each group of 13 consecutive weeks
#PLUM flavor does not come in DIET so we will assume that the DIET sales are the same as the REGULAR sales
sales_by_innovation <- df %>%
  filter(CATEGORY == "SSD",
         str_detect(ITEM, "PLUM"),
         str_detect(PACKAGE, "12"),
         str_detect(PACKAGE, 'ONE')) %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 13, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 13)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_innovation$week_label <- factor(sales_by_innovation$week_label, levels = sales_by_innovation$week_label[order(sales_by_innovation$WEEK)])
ggplot(sales_by_innovation, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 13-Week Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```

> With Package, Plum, and SSD we sales increaing slighltyly through the year with week 32 though 45 being the best.

## Make a new smaller "innovation" data frame



```{r}
#create innovation based on SSD, Plum, and packages that come in 12 (nearest number to 11) and ONE
innovation<- df %>%
  filter(CATEGORY == "SSD",
         str_detect(ITEM, "PLUM"),
         str_detect(PACKAGE, '12'),
         str_detect(PACKAGE, 'ONE'))

 


#unique PACKAGE string from innovation
print(unique(innovation$PACKAGE))


      
library(dplyr)
library(lubridate)

innovation <- innovation %>%
  mutate(
    MONTH = month(ymd(DATE)),  # Extract month using lubridate's ymd function
    MONTH = as.factor(MONTH)   # Convert the extracted month into a factor
  )

str(innovation)
print(unique(innovation$ITEM))

```


```{r}
# Count the number of unique PACKAGE column of our sample
table(innovation$PACKAGE)

```




```{r}
# Creating an 'innovation' data frame
model <- lm(DOLLAR_SALES ~ UNIT_SALES + CALORIC_SEGMENT + PACKAGE + SEASON + REGION, data = innovation)
summary(model)





```


> Good gravy 0.87 R-squared. This model is a beast. PACKAGE12SMALL 24ONE CUP and KANSAS are the most significant variables. Seasonally Spring and Summer are the most signficant and best times to sell. 


#More exploration

```{r}


library(dplyr)

small_group <- df %>%
  filter(UNIT_SALES < 600, DOLLAR_SALES < 3600)

skim(small_group)
```
> Our sample looks farily representative as the mean and sd are quite close to the full df for DIET SMASH.

```{r fig.width=10, fig.height=10}
# Create a scatter plot with the regression line, colored by MANUFACTURER
ggplot(small_group, aes(x = UNIT_SALES, y = DOLLAR_SALES, color = MANUFACTURER)) +
  geom_point(alpha = 0.5) +  # Adjust alpha to avoid overplotting, if necessary
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band for clarity
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES by MANUFACTURER",
       x = "UNTI SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position if needed

```


> Behold the realm of DIET SMASH. Certain items sell much better, or wosrse with consideration of slop of dollars to units sold. The overall trend line in this realm is below that of DIET SMASH, as DIET SMASH is almost $1000 at 200 units sold while the realm is 375 units sold to get to $1000.





#Make the small plum df
> Investigating drinks with Plum as a flavor in the Item description.

```{r}
# Create a new data frame with only the rows where the ITEM column contains the word 'plum'
plum_small <- df[grep("plum", df$ITEM, ignore.case = TRUE), ]

```

```{r}
skim(plum_small)
```

> Plum as a flavor has lower unit sales mean of 58 and dollar sales mean of $169 as compared to overall Diet Smash at 80 mean unit sales  and mean of $235 dolalr sales.


```{r}

# plum small is dataframe
model <- lm(DOLLAR_SALES ~ UNIT_SALES + CALORIC_SEGMENT + PACKAGE + CATEGORY + SEASON + REGION, data = plum_small)
summary(model)


```
> Adjusted R squared of 0.934, just amazing. Plum does well in Colorado, Kansas, and New Mexico (the last two being similar to Diet Smash). The two existing packages that Diet Smash comes in also are significant, for Plum (2L Multi JUG fairly strong negative like Diet Smash and 12small 12one CUP just slightly positive). The 4 week package run, PACKAGE12SMALL 6ONE CUP, from Diet Smash is not statisically signficant for Plum either, which could be cause of concern for what sounds to be a similar package of 11small 4one "JUG|CUP". Although "All Other Ones", could be a good proxy for innovation packaging and is strongly signficant. The only thing that is not significant across the board is the season for plum, although Summer and Winter could be edge cases thrown off by some noise. 




## Cleaning

> Reworking the subset plum for more feature engineering.

```{r}

plum_small <- plum_small %>%
  mutate(
    PACKAGE2 = str_extract(ITEM, "(CUP|JUG).*"),  # Extracts the part from CUP or JUG to the end.
    ITEM = str_replace(ITEM, "(CUP|JUG).*", "")  # Replaces the CUP/JUG and everything after it with empty string in ITEM.
  )


#plum_small
```

```{r}


plum_small <- plum_small %>%
  mutate(
    TEMP = str_extract(ITEM, "\\d+\\.?\\d*.*"), # Extracts the part from the first number to the end.
    PACKAGE2 = if_else(is.na(PACKAGE2), TEMP, paste(PACKAGE2, TEMP)), # Combines existing PACKAGE2 with new extraction if needed.
    ITEM = str_replace(ITEM, "\\d+\\.?\\d*.*", ""), # Removes the numeric part and everything after it from ITEM.
    TEMP = NULL  # Removes the temporary column.
  )

#plum_small
```

```{r}
na_rows <- plum_small %>%
  filter(is.na(PACKAGE2))
#na_rows
#the above steps excised all packaging out of ITEM column
```

```{r}



plum_small <- plum_small %>%
  mutate(
    GENTLE_DRINK = if_else(str_detect(ITEM, "GENTLE DRINK"), 1, 0), # Assigns 1 if "GENTLE DRINK" exists, otherwise 0.
    ITEM = str_replace(ITEM, "GENTLE DRINK", "") # Removes "GENTLE DRINK" from ITEM.
  )
#plum_small
```

```{r}

plum_small <- plum_small %>%
  mutate(
    ENERGY_DRINK = if_else(str_detect(ITEM, "ENERGY DRINK"), 1, 0), # Assigns 1 if "ENERGY DRINK" exists, otherwise 0.
    ITEM = str_replace(ITEM, "ENERGY DRINK", "") # Removes "ENERGY DRINK" from ITEM.
  )

#plum_small
```


```{r}
library(dplyr)
library(stringr)

# Define the pattern as a regular expression
pattern <- "ZERO CALORIES|ZERO CALORIE|ZERO SUGAR|SUGAR FREE|NO CALORIES"

plum_small <- plum_small %>%
  mutate(
    CALORIC_SEGMENT_TEXT = str_extract(ITEM, pattern), # Extracts matching text based on the pattern.
    ITEM = str_replace_all(ITEM, pattern, "") # Removes extracted text from ITEM.
  )

#plum_small
```


```{r}
library(dplyr)
library(stringr)

plum_small <- plum_small %>%
  mutate(
    CALORIC_SEGMENT_TEXT = if_else(str_detect(ITEM, "\\bDIET\\b"), 
                                   if_else(is.na(CALORIC_SEGMENT_TEXT), "DIET", paste(CALORIC_SEGMENT_TEXT, "DIET", sep=", ")), 
                                   CALORIC_SEGMENT_TEXT)
  )
#plum_small
```

```{r}


# Function to remove the second instance of any repeating word
remove_second_instance <- function(item) {
  words <- unlist(str_split(item, "\\s+")) # Split item into words
  unique_words <- unique(words) # Get unique words to check for repeats
  for (word in unique_words) {
    word_indices <- which(words == word) # Find all indices of the current word
    if (length(word_indices) > 1) { # If there is more than one occurrence
      words[word_indices[2]] <- "" # Remove the second occurrence
    }
  }
  return(paste(words, collapse = " ")) # Reconstruct sentence without the second instance
}

# Apply the function to the 'ITEM' column
plum_small <- plum_small %>%
  mutate(ITEM = sapply(ITEM, remove_second_instance))


# Remove specific columns
plum_small <- select(plum_small, -PACKAGE2, -GENTLE_DRINK, -ENERGY_DRINK, -CALORIC_SEGMENT_TEXT)

```

```{r}
head(plum_small)
```


```{r}

```

# FINAL THOUGHTS

> We now know that there are 2 innovation aspects at play here, a new package for Diet Smash in the complete sense of 11small and 4one and an existing flavor,net new added to Diet Smash. Both our innovation plum form and the small Plum  Multiple Linear regressions are incredibly high indicating that there is high potential some reasonable forecasting. Diet Smash has some significance in terms of season in Summer and Winter, but Plum by itself is right on the edge of not being significant for Summer and Winter. Our innovation data frame showed promised spring and summer. Packaging is not as strong by itself as Plum and Diet Smash ony comes in 2 regular types with 1 size that ran for 4 weeks. 12small 6one is likely pretty close for 11small 4one, but regressions in the innovation data frame showed PACKAGE12SMALL 24ONE CUP as the winner . It's possible that together, based on Diet Smash, that the best 13 weeks are Spring and Summer for Plum, with specific week details to be determined.




```{r}
df <- read_csv("swire_no_nas_w_pop.csv")  #inject the data and we will sub-sample
```

> Remove things at the very beginning and drop the weird missing period near the end

```{r}
#Group by ITEM with DATE Before 2021-01-01, drop those ITEM rows
# df_long_running <- df %>%
#   group_by(ITEM) %>%
#   filter(DATE <= "2021-01-01")
# 
# #remove all rows in plum_long_running from plum
# df <- df %>%
#   anti_join(df_long_running)

#Group by ITEM rows with less than 12 weeks of data
df_small <- df %>%
  group_by(ITEM) %>%
  filter(n() <= 12)

#remove all rows in df small that run less than 13 weeks 
df <- df %>%
  anti_join(df_small)

#Drop rows after May 21st 2023 as there are several gaps for most brands in innovation plum
df <- df %>%
  filter(DATE <= "2023-05-21")

#cleanup everything but df
rm(df_small)

#skim(df)


```

```{r}
#start with PLUM, SSD, and Package features
plum_package <- df %>%
  filter(CATEGORY == "SSD",
         str_detect(ITEM, "PLUM"))

#try plum ssd diet
plum_diet <- df %>%
  filter (
         str_detect(ITEM, "PLUM"),
         CALORIC_SEGMENT == "DIET/LIGHT")

#toss in some diet smash for good measure ans some package features
diet_smash <- df %>%
  filter (
         BRAND == "DIET SMASH",
         CATEGORY == "SSD")
         

#combine the three
merged_plum_innovation <- bind_rows(plum_package, plum_diet, diet_smash)

#remove duplicate rows
df <- merged_plum_innovation %>% distinct()

skim(df)

#cleanup all objects other than df
rm(plum_package, plum_diet, diet_smash, merged_plum_innovation)

```


```{r}

regions_joinme <- read.csv("states_summary.csv")

unique(regions_joinme$REGION)
# "NORTHERN"    "DESERT_SW"   "PRAIRIE"     "CALI_NEVADA"  "MOUNTAIN"    "SOCAL"   "ARIZONA"    "NEWMEXICO"   "NOCAL"    "COLORADO"    "KANSAS" 

str(regions_joinme)
# Perform a left join using the merge() function
df <- merge(df, regions_joinme[, c("MARKET_KEY", "REGION")], by = "MARKET_KEY", all.x = TRUE)
rm(regions_joinme)

```

```{r}
# Update CALORIC_SEGMENT values: 0 if 'DIET/LIGHT', otherwise 1
df$CALORIC_SEGMENT <- ifelse(df$CALORIC_SEGMENT == "DIET/LIGHT", 0, 1)
df$MARKET_KEY <- as.character(df$MARKET_KEY)
df <- df %>%
  mutate(
    MONTH = as.numeric(substr(DATE, 6, 7)),  # Extract the month from YYYY-MM-DD format
    SEASON = case_when(
      MONTH %in% c(12, 01, 02) ~ "WINTER",
      MONTH %in% c(03, 04, 05) ~ "SPRING",
      MONTH %in% c(06, 07, 08) ~ "SUMMER",
      MONTH %in% c(09, 10, 11) ~ "FALL",
      TRUE ~ NA_character_  # This is just in case there are any undefined values
    )
  )




```

# Cleaning Plum

```{r }
#save merged_innovation_df back to plum
plum  <- df

skim(plum)



```

> Reworking the subset plum for more feature engineering.

```{r}

plum <- plum %>%
  mutate(
    PACKAGE2 = str_extract(ITEM, "(CUP|JUG).*"),  # Extracts the part from CUP or JUG to the end.
    ITEM = str_replace(ITEM, "(CUP|JUG).*", "")  # Replaces the CUP/JUG and everything after it with empty string in ITEM.
  )



```

```{r}


plum <- plum %>%
  mutate(
    TEMP = str_extract(ITEM, "\\d+\\.?\\d*.*"), # Extracts the part from the first number to the end.
    PACKAGE2 = if_else(is.na(PACKAGE2), TEMP, paste(PACKAGE2, TEMP)), # Combines existing PACKAGE2 with new extraction if needed.
    ITEM = str_replace(ITEM, "\\d+\\.?\\d*.*", ""), # Removes the numeric part and everything after it from ITEM.
    TEMP = NULL  # Removes the temporary column.
  )

```

```{r}
na_rows <- plum %>%
  filter(is.na(PACKAGE2))
na_rows
#the above steps excised all packaging out of ITEM column
```

```{r}

# Function to remove the second instance of any repeating word
remove_second_instance <- function(item) {
  words <- unlist(str_split(item, "\\s+")) # Split item into words
  unique_words <- unique(words) # Get unique words to check for repeats
  for (word in unique_words) {
    word_indices <- which(words == word) # Find all indices of the current word
    if (length(word_indices) > 1) { # If there is more than one occurrence
      words[word_indices[2]] <- "" # Remove the second occurrence
    }
  }
  return(paste(words, collapse = " ")) # Reconstruct sentence without the second instance
}

# Apply the function to the 'ITEM' column
plum <- plum %>%
  mutate(ITEM = sapply(ITEM, remove_second_instance))

```

```{r }
# #One hot encode either "ENERGY" or "ED" in ITEM as an ENERGY_DRINK
plum$ENERGY_DRINK <- ifelse(str_detect(plum$ITEM, "ENERGY|' ED'"), 1, 0)

plum$ITEM <- str_replace(plum$ITEM, "ENERGY DRINK", "")
plum$ITEM <- str_replace(plum$ITEM, "ENERGY", "")
plum$ITEM <- str_replace(plum$ITEM, " ED", "")
table(plum$ENERGY_DRINK)

table(plum$CATEGORY)

plum %>%
  filter(ENERGY_DRINK == 1,
         CATEGORY=='SSD') %>%
  select(ITEM) %>%
  head(10)

```

```{r}
# Remove specific columns
#plum <- select(plum, -PACKAGE2, -CATEGORY)

```

```{r}
head(plum)
table(plum$ITEM)

```

```{r}
#Trim trailing white space at end of ITEM
plum$ITEM <- str_trim(plum$ITEM, side = "right")

```

```{r}
# #replace "ENERGY DRINK" with "" in ITEM
plum$ITEM <- str_replace(plum$ITEM, "GENTLE DRINK", "")

```

```{r }

pattern <- "ZERO CALORIES|ZERO CALORIE|ZERO SUGAR|SUGAR FREE|NO CALORIES|ZERO CARB|PURE ZERO|DIET|NO SWEETENERS|ZERO CAL PER|CALORIE FREE"

plum <- plum %>%
  mutate(
    CALORIC_SEGMENT_TEXT = str_extract(ITEM, pattern), # Extracts matching text based on the pattern.
    ITEM = str_replace_all(ITEM, pattern, "") # Removes extracted text from ITEM.
  )

```

```{r}
# Find the minimum launch date for each product
min_launch_dates <- plum %>%
  group_by(ITEM) %>%
  summarise(min_launch_date = min(DATE))

# Join the minimum launch dates back to the original data
plum <- plum %>%
  left_join(min_launch_dates, by = "ITEM")

# Calculate the number of weeks since the product launch
plum <- plum %>%
  mutate(WEEKS_SINCE_LAUNCH = as.numeric(difftime(DATE, min_launch_date, units = "weeks")))

# Selecting required columns and printing the first 10 rows
plum %>%
  filter(UNIT_SALES > 0) %>%
  select(DATE, ITEM, WEEKS_SINCE_LAUNCH) %>%
  head(10)



#Set any negative WEEKS_SINCE_LAUNCH to 0
# plum <- plum %>%
#   mutate(WEEKS_SINCE_LAUNCH = ifelse(WEEKS_SINCE_LAUNCH < 0, 0, WEEKS_SINCE_LAUNCH))


```

```{r}
#too many brands and packages, lets try removing ITEM or CATEGORIES of WATER, ENERGY, and JUG packages
plum <- plum %>% 
  filter(!grepl("WATER", ITEM),
         !CATEGORY=='SPARKLING WATER',
         !CATEGORY=='ING ENHANCED WATER',
         !CATEGORY=='ENERGY')

#Let's remove Brand Diet Smash
# plum <- plum %>% 
#   filter(!BRAND=='DIET SMASH')


```


```{r}

print(unique(plum$ITEM))
print(unique(plum$BRAND))
print(unique(plum$CATEGORY))
print(unique(plum$PACKAGE))
```
```{r }
write_csv(plum, "plum_tableau.csv")

str(plum)

```



```{r}
#remove all objects other than plum
rm(list = setdiff(ls(), "plum"))

print(unique(plum$ITEM))
print(unique(plum$BRAND))
print(unique(plum$CATEGORY))
print(unique(plum$PACKAGE))
 
skim(plum)

```


```{r}
# Creating an 'innovation' data frame

#factor all character variables
plum$REGION <- as.factor(plum$REGION)
#plum$CATEGORY <- as.factor(plum$CATEGORY)
plum$BRAND <- as.factor(plum$BRAND)
plum$SEASON <- as.factor(plum$SEASON)
plum$PACKAGE2 <- as.factor(plum$PACKAGE2)
sapply(plum, function(x) sum(is.na(x)))

model <- lm(DOLLAR_SALES ~ UNIT_SALES  + POP_SQMI + REGION  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = plum)
summary(model)


```

```{r}
# Creating an 'innovation' data frame

model <- lm(UNIT_SALES ~ DOLLAR_SALES + PACKAGE + POP_SQMI + REGION   + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = plum)
summary(model)

```

```{r}
# Creating an 'innovation' data frame

model <- lm(UNIT_SALES ~  CALORIC_SEGMENT + PACKAGE + POP_SQMI + REGION   + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = plum)
summary(model)

```

```{r}
#weekly sales where ITEM contains "RAINING THRASHED PLUM"
plum %>%
  filter(str_detect(ITEM, "RAINING  THRASHED PLUM")) %>%
  group_by(DATE) %>%
  summarise(UNIT_SALES = sum(UNIT_SALES)) %>%
  ggplot(aes(x = DATE, y = UNIT_SALES)) +
  geom_line() +
  labs(title = "Weekly Sales of 'RAINING THRASHED PLUM'", x = "Week of Year", y = "Unit Sales")

```



```{r}
# Load and prepare dataset
df <- read.csv("plum_tableau.csv") 
# Load and prepare dataset 

str(df)

plum <- df %>% 
  #select(-DATE, -MONTH, -SEASON, -BRAND, -REGION, -ITEM )
  select(-MONTH, -SEASON, -min_launch_date, -PACKAGE2, -CALORIC_SEGMENT_TEXT)

```





```{r}

print(unique(plum$ITEM))
print(unique(plum$BRAND))
print(unique(plum$CATEGORY))
print(unique(plum$PACKAGE))
```



```{r}
# Assuming plum is your data frame and PACKAGING is the column of interest

# Create new columns in woodsy for each unique substring
# Each column will have a 1 if the substring is found in the PACKAGING column, 0 otherwise

plum$`12SMALL 12ONE CUP` = as.integer(grepl("12SMALL 12ONE CUP", plum$PACKAGE))
plum$`20SMALL MULTI JUG` = as.integer(grepl("20SMALL MULTI JUG", plum$PACKAGE))
plum$`12SMALL 6ONE CUP` = as.integer(grepl("12SMALL 6ONE CUP", plum$PACKAGE))
plum$`12SMALL 24ONE CUP` = as.integer(grepl("12SMALL 24ONE CUP", plum$PACKAGE))
plum$`12SMALL MLT PLASTICS JUG` = as.integer(grepl("12SMALL MLT PLASTICS JUG", plum$PACKAGE))
plum$`.5L 6ONE JUG` = as.integer(grepl(".5L 6ONE JUG", plum$PACKAGE))
plum$`12SMALL 20ONE CUP` = as.integer(grepl("12SMALL 20ONE CUP", plum$PACKAGE))
plum$`12SMALL 18ONE CUP` = as.integer(grepl("12SMALL 18ONE CUP", plum$PACKAGE))
plum$`12SMALL 24ONE PLASTICS JUG` = as.integer(grepl("12SMALL 24ONE PLASTICS JUG", plum$PACKAGE))

     

#one hot encode non brand ITEM strings
#"RAINING THRASHED PLUM"  "BEAUTIFUL GREENER PLUM" "ZIZZLES PLUM"     
plum$`RAINING THRASHED PLUM` = as.integer(grepl("RAINING THRASHED PLUM", plum$ITEM))
plum$`BEAUTIFUL GREENER PLUM` = as.integer(grepl("BEAUTIFUL GREENER PLUM", plum$ITEM))
plum$`ZIZZLES PLUM` = as.integer(grepl("ZIZZLES PLUM", plum$ITEM))

# Print the head of the data frame to see the first few rows
head(plum)

plum$CATEGORY <- NULL
plum$MARKET_KEY <- NULL
plum$MANUFACTURER <- NULL
plum$PACKAGE <- NULL


```

```{r}
library(fastDummies)

# One-hot encode the specified columns
plum <- fastDummies::dummy_cols(plum, select_columns = c("REGION", "ITEM"), remove_selected_columns = TRUE)
#plum <- fastDummies::dummy_cols(plum, select_columns = c("REGION", "SEASON","ITEM"), remove_selected_columns = TRUE)

# View the first few rows to verify the changes
head(plum)




write.csv(plum, "plum_one_hot.csv", row.names = FALSE)

```

```{r}

library(fastDummies)

# One-hot encode
plum <- fastDummies::dummy_cols(plum, select_columns = "BRAND", remove_selected_columns = TRUE)

# View the first few rows to verify
head(plum)

```


```{r}

#create new week of year column

plum <- plum %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK_OF_YEAR = lubridate::week(DATE))

#Drop DATE column
plum$DATE <- NULL

```





```{r}
# Summarize the dataset
skimr::skim(plum)
```

```{r}
#remove top 10 percent of unit sales to clean up outliers
df <- plum %>% 
  filter(UNIT_SALES < quantile(UNIT_SALES, 0.99))

```










```{r}


# Split the data
set.seed(123)
df_testtrn <- initial_split(df, prop = 0.8, strata = UNIT_SALES)
Train <- training(df_testtrn)
Test <- testing(df_testtrn)

# Prepare features and labels for XGBoost
train_features <- Train[, -which(names(Train) == "UNIT_SALES")]
train_labels <- Train$UNIT_SALES
test_features <- Test[, -which(names(Test) == "UNIT_SALES")]
test_labels <- Test$UNIT_SALES

# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(train_features), label = train_labels)
dtest <- xgb.DMatrix(data = as.matrix(test_features), label = test_labels)

```

```{r}

# Define XGBoost parameters
set.seed(123)
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.05,
  max_depth = 4,
  min_child_weight = 3,
  subsample = 0.7,
  colsample_bytree = 0.6,
  lambda = 1,
  alpha = 1
)

```

```{r}

# Perform cross-validation to find the optimal number of boosting rounds
cv_results <- xgb.cv(
  params = params,
  data = dtrain,  
  nfold = 5,
  nrounds = 500,  # Changed from 'num_boost_round' to 'nrounds'
  early_stopping_rounds = 10,
  metrics = "rmse",
  seed = 123
)
best_nrounds <- cv_results$best_iteration


```


```{r}
# Train the final model using the best number of rounds found
model_xgb <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)

```



```{r}

# Make predictions and evaluate the model
train_pred <- predict(model_xgb, dtrain)
test_pred <- predict(model_xgb, dtest)
train_rmse <- sqrt(mean((train_labels - train_pred)^2))
test_rmse <- sqrt(mean((test_labels - test_pred)^2))


```


```{r}

# Calculate R-squared for the training set
sst_train <- sum((train_labels - mean(train_labels)) ^ 2)
ssr_train <- sum((train_labels - train_pred) ^ 2)
r_squared_train <- 1 - (ssr_train / sst_train)

# Calculate R-squared for the test set
sst_test <- sum((test_labels - mean(test_labels)) ^ 2)
ssr_test <- sum((test_labels - test_pred) ^ 2)
r_squared_test <- 1 - (ssr_test / sst_test)

train_mape <- mean(abs((train_labels - train_pred) / train_labels)) * 100
test_mape <- mean(abs((test_labels - test_pred) / test_labels)) * 100
train_mae <- mean(abs(train_labels - train_pred))
test_mae <- mean(abs(test_labels - test_pred))



```


```{r}

cat("Model Performance Metrics:\n",
    "--------------------------\n",
    "Training RMSE: ", train_rmse, "\n",
    "Test RMSE: ", test_rmse, "\n",
    "Training R-squared: ", r_squared_train, "\n",
    "Test R-squared: ", r_squared_test, "\n",
    "Training MAE: ", train_mae, "\n",
    "Test MAE: ", test_mae, "\n",
    "Training MAPE: ", train_mape, "%\n",
    "Test MAPE: ", test_mape, "%\n", sep="")


```



```{r}
# Correcting Residuals Data Frame
# Assuming 'train_labels' and 'test_labels' contain the actual values,
# and 'train_pred' and 'test_pred' contain your model's predictions:

residuals_train <- train_labels - train_pred
residuals_test <- test_labels - test_pred

residuals_data <- data.frame(
  Residuals = c(residuals_train, residuals_test),
  Dataset = c(rep('Training', length(residuals_train)), rep('Test', length(residuals_test)))
)

# Now plotting residuals with corrected data
ggplot(residuals_data, aes(x = Residuals, fill = Dataset)) +
  geom_histogram(binwidth = 1, position = 'identity', alpha = 0.6) +
  facet_wrap(~ Dataset) +
  ggtitle('Residuals Distribution')


```



```{r}
# Assuming train_labels, test_labels, train_pred, and test_pred are correctly defined

# Adjusted Actual vs. Predicted Data Preparation
actual_pred_data <- data.frame(
  Actual = c(train_labels, test_labels),
  Predicted = c(train_pred, test_pred),
  Dataset = c(rep('Training', length(train_labels)), rep('Test', length(test_labels)))
)

# Plotting Actual vs. Predicted Values
ggplot(actual_pred_data, aes(x = Actual, y = Predicted, colour = Dataset)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed', color = 'red') +
  xlab('Actual Values') +
  ylab('Predicted Values') +
  scale_colour_manual(values = c('Training' = 'blue', 'Test' = 'red')) +
  ggtitle('Actual vs. Predicted Values')




```

```{r}


library(xgboost)

# Calculate feature importance
importance_matrix <- xgb.importance(feature_names = colnames(train_features), model = model_xgb)

# View the feature importance scores
print(importance_matrix)

```

```{r fig.width = 10, fig.height= 8}
# Plot the feature importance
xgb.plot.importance(importance_matrix = importance_matrix)

```

```{r fig.width=10, fig.height=8}
# Compute partial dependence data for 'DOLLAR_SALES' and 'plum', CALORIC_SEGMENT, and "ENERGY
# pd <- partial(model_xgb, pred.var = c("DOLLAR_SALES", "plum", "CALORIC_SEGMENT", ENERGY"), train = train_features, grid.resolution = 20)
# 
# # Default PDP
# pdp1 <- plotPartial(pd, plot = TRUE)
# 
# # Add contour lines and use a different color palette
# rwb <- colorRampPalette(c("red", "white", "blue"))
# pdp2 <- plotPartial(pd, contour = TRUE, col.regions = rwb)
# 
# # 3-D surface
# pdp3 <- plotPartial(pd, levelplot = FALSE, zlab = "Predicted Outcome", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
# 
# # Combine plots into one window
# grid.arrange(pdp1, pdp2, pdp3, ncol = 3)

```

## XGBOOST Model #2 
>Model with NO DOLLAR SALES Variable

```{r}
# Assuming 'df' is your complete dataframe and 'UNIT_SALES' is your target variable
df2 <- df
# Remove DOLLAR_SALES from the features
df2$DOLLAR_SALES <- NULL

# Split the updated data into training and testing sets (assuming you're using a similar approach as before)
set.seed(123)
df2_testtrn <- initial_split(df2, prop = 0.8, strata = UNIT_SALES)
Train <- training(df2_testtrn)
Test <- testing(df2_testtrn)

# Prepare features and labels for XGBoost, excluding DOLLAR_SALES
train_features <- Train[, -which(names(Train) == "UNIT_SALES")]
train_labels <- Train$UNIT_SALES
test_features <- Test[, -which(names(Test) == "UNIT_SALES")]
test_labels <- Test$UNIT_SALES

# Convert data to DMatrix format for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_features), label = train_labels)
dtest <- xgb.DMatrix(data = as.matrix(test_features), label = test_labels)

```

```{r}
# Assuming 'params' and 'best_nrounds' are defined as before

# Train the final model without DOLLAR_SALES
model_xgb_no_dollar_sales <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)

```

```{r}


# Make predictions and evaluate the model
train_pred <- predict(model_xgb_no_dollar_sales, dtrain)
test_pred <- predict(model_xgb_no_dollar_sales, dtest)
train_rmse <- sqrt(mean((train_labels - train_pred)^2))
test_rmse <- sqrt(mean((test_labels - test_pred)^2))


```


```{r}

# Calculate R-squared for the training set
sst_train <- sum((train_labels - mean(train_labels)) ^ 2)
ssr_train <- sum((train_labels - train_pred) ^ 2)
r_squared_train <- 1 - (ssr_train / sst_train)

# Calculate R-squared for the test set
sst_test <- sum((test_labels - mean(test_labels)) ^ 2)
ssr_test <- sum((test_labels - test_pred) ^ 2)
r_squared_test <- 1 - (ssr_test / sst_test)

train_mape <- mean(abs((train_labels - train_pred) / train_labels)) * 100
test_mape <- mean(abs((test_labels - test_pred) / test_labels)) * 100
train_mae <- mean(abs(train_labels - train_pred))
test_mae <- mean(abs(test_labels - test_pred))



```

```{r}
# Correcting Residuals Data Frame
# Assuming 'train_labels' and 'test_labels' contain the actual values,
# and 'train_pred' and 'test_pred' contain your model's predictions:

residuals_train <- train_labels - train_pred
residuals_test <- test_labels - test_pred

residuals_data <- data.frame(
  Residuals = c(residuals_train, residuals_test),
  Dataset = c(rep('Training', length(residuals_train)), rep('Test', length(residuals_test)))
)

# Now plotting residuals with corrected data
ggplot(residuals_data, aes(x = Residuals, fill = Dataset)) +
  geom_histogram(binwidth = 1, position = 'identity', alpha = 0.6) +
  facet_wrap(~ Dataset) +
  ggtitle('Residuals Distribution')


```

```{r}
# Assuming train_labels, test_labels, train_pred, and test_pred are correctly defined

# Adjusted Actual vs. Predicted Data Preparation
actual_pred_data <- data.frame(
  Actual = c(train_labels, test_labels),
  Predicted = c(train_pred, test_pred),
  Dataset = c(rep('Training', length(train_labels)), rep('Test', length(test_labels)))
)

# Plotting Actual vs. Predicted Values
ggplot(actual_pred_data, aes(x = Actual, y = Predicted, colour = Dataset)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed', color = 'red') +
  xlab('Actual Values') +
  ylab('Predicted Values') +
  scale_colour_manual(values = c('Training' = 'blue', 'Test' = 'red')) +
  ggtitle('Actual vs. Predicted Values')




```

```{r}

cat("Model Performance Metrics:\n",
    "--------------------------\n",
    "Training RMSE: ", train_rmse, "\n",
    "Test RMSE: ", test_rmse, "\n",
    "Training R-squared: ", r_squared_train, "\n",
    "Test R-squared: ", r_squared_test, "\n",
    "Training MAE: ", train_mae, "\n",
    "Test MAE: ", test_mae, "\n",
    "Training MAPE: ", train_mape, "%\n",
    "Test MAPE: ", test_mape, "%\n", sep="")


```




```{r}

# Calculate feature importance
importance_matrix2 <- xgb.importance(feature_names = colnames(train_features), model = model_xgb_no_dollar_sales)

# View the feature importance scores
print(importance_matrix2)
```

```{r fig.width= 10, fig.height= 8}
xgb.plot.importance(importance_matrix = importance_matrix2)
```



```{r}
if (!requireNamespace("pdp", quietly = TRUE)) install.packages("pdp")
if (!requireNamespace("xgboost", quietly = TRUE)) install.packages("xgboost")
library(pdp)
library(xgboost)

```




```{r}
pdp::partial(model_xgb_no_dollar_sales, pred.var = "WEEK_OF_YEAR", train = train_features)

pd <- partial(model_xgb_no_dollar_sales, pred.var = "WEEK_OF_YEAR", train = train_features, grid.resolution = 20)

# Default PDP
pdp1 <- plotPartial(pd, plot = TRUE)



# plot
grid.arrange(pdp1)
```

> Based on the plum Energy Drink innovation datafram we expect the best 13 weeks to be
between about weeks 33 and weeks 46.


```{r}
#rename columes to match original features

dummy_data <- dummy_data %>%
  rename(
    `BEAUTIFUL.GREENER` = `BEAUTIFUL GREENER`,   
    `X12SMALL.12ONE.CUP` = `12SMALL 12ONE CUP`,                                        `HILL.MOISTURE.THRASHED.APPLE` = `HILL MOISTURE THRASHED APPLE`,
    `BEAUTIFUL.GREENER.PLUM.` = `BEAUTIFUL GREENER PLUM `,
    `X12SMALL.24ONE.CUP` = `12SMALL 24ONE CUP`,
    `SINGLE.GROUP` = `SINGLE GROUP`,
    `ZIZZLES.PLUM.` = `ZIZZLES PLUM `,
    `X12SMALL.6ONE.CUP` = `12SMALL 6ONE CUP`,
    `X12SMALL.20ONE.CUP` = `12SMALL 20ONE CUP`,
     `RAINING.THRASHED.PLUM.`=`RAINING THRASHED PLUM `,
    `DIET.SMASH` =`DIET.SMASH`,
    `DIET.SMASH.SUNSET..` =`DIET SMASH SUNSET  `,
    `X12SMALL.18ONE.CUP`=`X12SMALL 18ONE CUP`,
    
  )

# Check for Matching Features
#Get the column names of Test and dummy_data
names_Test <- names(Test)
names_dummy_data <- names(dummy_data)

# Find the matching column names
matching_names <- intersect(names_Test, names_dummy_data)

# Find the non-matching column names
non_matching_names_Test <- setdiff(names_Test, matching_names)
non_matching_names_dummy_data <- setdiff(names_dummy_data, matching_names)

#Print the matching and non-matching column names
cat("Matching column names:", paste(matching_names, collapse = ", "), "\n")
cat("Non-matching column names in Test:", paste(non_matching_names_Test, collapse = ", "), "\n")
cat("Non-matching column names in dummy_data:", paste(non_matching_names_dummy_data, collapse = ", "), "\n")


# Adding the non-matching columns to dummy_data with default values
for (col in non_matching_names_Test) {
  dummy_data[[col]] <- NA  # You can change NA to any default value you prefer
}


# Get the column names of the Test dataframe
test_colnames <- colnames(Test)

# Reorder columns of dummy_data to match the order of columns in Test
dummy_data <- dummy_data %>%
  select(all_of(test_colnames))

# Prepare features for XGBoost
dummy_features <- dummy_data[, -which(names(dummy_data) == "UNIT_SALES")]


# Convert data to DMatrix format

dummy_dmatrix<- xgb.DMatrix(data = as.matrix(dummy_features))

dummy_pred <- predict(model_xgb, dummy_dmatrix)

# Add the predictions to dummy_data
dummy_data$Predictions <- dummy_pred

# Convert predictions to integers
dummy_data$Predictions <- round(dummy_pred)

# Convert to integer data type
dummy_data$Predictions <- as.integer(dummy_data$Predictions)

summary(dummy_data$Predictions)

ggplot(dummy_data, aes(x = Predictions)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Predicted Values",
       x = "Predicted Values",
       y = "Density")

```



```{r}
#cleanup all obj
rm(list = ls())
```

> Dummy data for unseen innovation products does not provide the most confident results.

---
title: "Sampling Swire for Coke Heroes"
author: "Jake, Ian, Andrew, Michael"
date:  "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    css: custom-styles.css
    theme: null
    highlight: null
    toc: true
    toc_float: false
---

#Reference Q information for this flava flav
> Item Description: Diet Energy Moonlit Casava 2L Multi Jug
Caloric Segment: Diet
Market Category: Energy
Manufacturer: Swire-CC
Brand: Diet Moonlit
Package Type: 2L Multi Jug
Flavor: ‘Cassava’
Swire plans to release this product for 6 months. What will the forecasted demand be, in weeks, for this product?





## Taking a sample of the whole dataset

```{r}
df <- readRDS("swire_no_nas.rds")  #inject the data and we will sub-sample


```

```{r}

regions_joinme <- read.csv("states_summary.csv")

unique(regions_joinme$REGION)
# "NORTHERN"    "DESERT_SW"   "PRAIRIE"     "CALI_NEVADA"  "MOUNTAIN"    "SOCAL"   "ARIZONA"    "NEWMEXICO"   "NOCAL"    "COLORADO"    "KANSAS"

str(regions_joinme)
# Perform a left join using the merge() function
df <- merge(df, regions_joinme[, c("MARKET_KEY", "REGION")], by = "MARKET_KEY", all.x = TRUE)
rm(regions_joinme)

library(tidyverse)
zip <- read.csv("USA_Zip_Code_Boundaries.csv")
z2m <- read.csv("zip_to_market_unit_mapping.csv")
#only select ZIP Code and SQMI
zip <- zip[,c("ZIP_CODE", "SQMI")]
#merge zip and z2m based on zip_code
market_sqmi <- merge(zip, z2m, by.x="ZIP_CODE", by.y="ZIP_CODE")
#aggregate MARKET_KEY and sum up all SQMI for every ZIP_CODE in that MARKET_KEY
market_sqmi <- market_sqmi %>%
  group_by(MARKET_KEY) %>%
  summarise(SQMI = sum(SQMI))

#load pop from demo data
demo <- read.csv("final_market_demographics_wide_use_me.csv")
#create new demo with only TOTAL_POP_COUNT aggregated by MARKET_KEY
demo <- demo %>%
  group_by(MARKET_KEY) %>%
  summarise(TOTAL_POP_COUNT = sum(TOTAL_POP_COUNT))
#merge market_sqmi and demo based on MARKET_KEY, only select "TOTAL_POP_COUNT" from demo
market_key_pop <- merge(demo[,c("MARKET_KEY", "TOTAL_POP_COUNT")],market_sqmi,  by = "MARKET_KEY")
#create new column POP_SQMI by dividing TOTAL_POP_COUNT by SQMI, drop TOTAL_POP_COUNT and SQMI
market_key_pop <- market_key_pop %>%
  mutate(POP_SQMI_MK = TOTAL_POP_COUNT/SQMI) %>%
  select(-c(TOTAL_POP_COUNT, SQMI))

#aggregate count of rows grouped by unique market key and save to new df
market_key_rows <- df %>%
  group_by(MARKET_KEY) %>%
  summarise(TRANSACTIONS_MK = n())



# Perform a left join using the merge() function on market key from df to market_key*
df <- merge(df, market_key_pop[, c("MARKET_KEY", "POP_SQMI_MK")], by = "MARKET_KEY", all.x = TRUE)
df <- merge(df, market_key_rows, by = "MARKET_KEY", all.x = TRUE)

#Transaction Density (TD) per captia per square mile
df <- df %>%
  mutate(TD = TRANSACTIONS_MK/POP_SQMI_MK)

#Unit Sales per Market Key Transactions - USPMKT
df <- df %>%
  mutate(USPMKT = UNIT_SALES/TRANSACTIONS_MK)

#New Column that describes the sum of unit sales per category by market key
UNIT_SALES_BY_CAT_MK <- df %>%
  select(MARKET_KEY, CATEGORY, UNIT_SALES) %>% 
  group_by(MARKET_KEY, CATEGORY) %>%
  summarise(UNIT_SALES_PER_CAT_MK = sum(UNIT_SALES)) 

MARKET_SHARE <- UNIT_SALES_BY_CAT_MK %>%
  mutate(MARKET_SHARE = UNIT_SALES/UNIT_SALES_PER_CAT_MK)
  
#Merge UNIT_SALES_BY_CAT_MK on MARKET_KEY and CATEGORY, add UNIT_SALES_PER_CAT_MK to df
df <- merge(df, UNIT_SALES_BY_CAT_MK, by = c("MARKET_KEY", "CATEGORY"), all.x = TRUE)

df <- df %>%
  mutate(MARKET_SHARE = UNIT_SALES/UNIT_SALES_PER_CAT_MK)

str(MARKET_SHARE)


str(df)
```




### Quick imputations 


```{r}
# Update CALORIC_SEGMENT values: 0 if 'DIET/LIGHT', otherwise 1
df$CALORIC_SEGMENT <- ifelse(df$CALORIC_SEGMENT == "DIET/LIGHT", 0, 1)
df$MARKET_KEY <- as.character(df$MARKET_KEY)
df <- df %>%
  mutate(
    MONTH = as.numeric(substr(DATE, 6, 7)),  # Extract the month from YYYY-MM-DD format
    SEASON = case_when(
      MONTH %in% c(12, 01, 02) ~ "WINTER",
      MONTH %in% c(03, 04, 05) ~ "SPRING",
      MONTH %in% c(06, 07, 08) ~ "SUMMER",
      MONTH %in% c(09, 10, 11) ~ "FALL",
      TRUE ~ NA_character_  # This is just in case there are any undefined values
    )
  )




```


```{r}
str(df)
```

## Making a 10% sample of the data to shrink it 

```{r}
# Assuming df is your dataframe
set.seed(123) # Set a random seed for reproducibility
sampled_df <- df[sample(1:nrow(df), 2446143), ]
rm(df)
```

```{r}
df <- sampled_df
rm(sampled_df)
```

```{r}
#skim(df)
```

```{r} 
summary(df)
```

### Linear model on sampled data looks the same largely 

```{r}



# Perform a linear regression with UNIT_SALES as the dependent variable
# and PRICE (or your chosen variable) as the independent variable
linear_model <- lm(DOLLAR_SALES ~ UNIT_SALES, data = df)

# Print the summary of the linear model to see the results
summary(linear_model)

```

```{r fig.width=10, fig.height=10}
# Create a scatter plot with the regression line, colored by MANUFACTURER
ggplot(df, aes(x = UNIT_SALES, y = DOLLAR_SALES, color = MANUFACTURER)) +
  geom_point(alpha = 0.5) +  # Adjust alpha to avoid overplotting, if necessary
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band for clarity
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES by MANUFACTURER",
       x = "UNIT SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position if needed

```

## Taking a look at Diet Moonlit brand..

```{r }

# create a table of total values by brand
brand_summary <- df %>%
  group_by(BRAND) %>%
  summarise(
    total_units_sold = sum(UNIT_SALES),
    total_revenue = sum(DOLLAR_SALES),
    avg_price = total_revenue / total_units_sold,
    total_days_sold = n() # Count the number of rows for each brand
  ) %>%
  arrange(desc(total_revenue)) %>%  # Order by revenue in descending order
  mutate(rank = row_number()) 

summary(brand_summary)

print(brand_summary[brand_summary$BRAND == "DIET MOONLIT", ])


```

> Diet Moonlit is a rising star ranking 69 out of 288 brands in terms of total revenue, with an average price of $3.50 slightly above the overall mean of $3.27.

```{r fig.height=10, fig.width=10}

# Filter the dataframe for only 'DIET SMASH'
filtered_df <- df %>% 
  filter(BRAND == "DIET MOONLIT")

# Create the plot
ggplot(filtered_df, aes(x = UNIT_SALES, y = DOLLAR_SALES)) +
  geom_point(color = "red", alpha = 1) +  # Bright red points with full opacity
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES for DIET SMASH",
       x = "UNIT SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "none")  


```

> DIET MOONLIT has a tight cluster below 1,000 unit sales and $2,500 revenue, but there are some remarkable high fliers nearing $20,000 and just over 3000 units.

# Sales by Week of the Year

```{r}
filtered_df %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  ggplot(aes(x = WEEK, y = total_sales)) +
  geom_line(color = "black") +  # Blue line connecting points
  labs(title = "Total Sales by Week of the Year",
       x = "Week of the Year",
       y = "Total Unit Sales") +
  theme_minimal()
```
> DIET MOONLIT shows many peaks and valleys in sales by week.

```{r}
library(zoo)
# Calculate total sales for each group of 211 consecutive weeks (6 months)
sales_by_group <- filtered_df %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 21, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 21)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_group$week_label <- factor(sales_by_group$week_label, levels = sales_by_group$week_label[order(sales_by_group$WEEK)])
ggplot(sales_by_group, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 6-month Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```
> DIET MOONLIT has it's best 6 month runs week 7 - 27 historically.

```{r }
#find the best 21 weeks for Casava sales
# Calculate total sales for each group of 21 consecutive weeks
sales_by_casava <- df %>%
  filter(str_detect(ITEM, "CASAVA")) %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 21, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 21)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_casava$week_label <- factor(sales_by_casava$week_label, levels = sales_by_casava$week_label[order(sales_by_casava$WEEK)])
ggplot(sales_by_casava, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 21-Week Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```
> Casava sales are best in the 21 weeks from week 14 to 34.


```{r }
#find the best 21 weeks for casava, energy, diet, s
# Calculate total sales for each group of 21 consecutive weeks

sales_by_innovation <- df %>%
  filter(CATEGORY == "ENERGY",
         str_detect(ITEM, "CASAVA")) %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK = as.integer(format(DATE, "%U"))) %>%
  group_by(WEEK) %>%
  summarise(total_sales = sum(UNIT_SALES)) %>%
  mutate(sales_in_group = rollsum(total_sales, 13, align = "left", fill = NA)) %>%
  mutate(week_label = paste0("Week ", WEEK + 1, " to Week ", WEEK + 13)) %>%
  arrange(WEEK) %>%  # Order by WEEK
  filter(!is.na(sales_in_group))  # Remove rows with sales_in_group = NA

# Plot the bar chart
sales_by_innovation$week_label <- factor(sales_by_innovation$week_label, levels = sales_by_innovation$week_label[order(sales_by_innovation$WEEK)])
ggplot(sales_by_innovation, aes(x = factor(week_label), y = sales_in_group)) +
  geom_bar(stat = "identity", fill = "black") +
  labs(title = "Total Sales for Each 13-Week Grouping",
       x = "Weeks (Starting from Week 1)",
       y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```


# Make a new smaller "innovation" data frame



```{r}
#create innovation based on Energy, Casava
innovation<- df %>%
  filter(CATEGORY == "ENERGY",
         str_detect(ITEM, "CASAVA"))


#unique PACKAGE string from innovation
print(unique(innovation$PACKAGE))


      
library(dplyr)
library(lubridate)

innovation <- innovation %>%
  mutate(
    MONTH = month(ymd(DATE)),  # Extract month using lubridate's ymd function
    MONTH = as.factor(MONTH)   # Convert the extracted month into a factor
  )

str(innovation)
print(unique(innovation$ITEM))

```


```{r}
# Count the number of unique PACKAGE column of our sample
table(innovation$PACKAGE)

```

```{r}
# Creating an 'innovation' data frame
#factor Region
innovation$REGION <- as.factor(innovation$REGION)
model <- lm(DOLLAR_SALES ~ UNIT_SALES + CALORIC_SEGMENT + PACKAGE + MARKET_SHARE, data = innovation)
summary(model)





```
> Cassava and Energy together do quite well (not possible to also add in DIET, but we will expect that folks that like Cassava Regular Energy will also like DIET). R2 of 0.98. Summer is statistically signficant, but negatively correlated with sales. 
SOCAL and NOCAL are significant in the positive direction.

#More exploration

```{r}


library(dplyr)

small_group <- df %>%
  filter(UNIT_SALES < 7000, DOLLAR_SALES < 20000)

skim(small_group)
skim(df %>% filter(BRAND == "DIET MOONLIT"))

```
> Our small df has a higher mean of unit sales and dollar sales of 149 and $496. as compared to the full df of DIET MOONLIT of 98 and $344.



```{r fig.width=10, fig.height=10}
# Create a scatter plot with the regression line, colored by MANUFACTURER
ggplot(small_group, aes(x = UNIT_SALES, y = DOLLAR_SALES, color = MANUFACTURER)) +
  geom_point(alpha = 0.5) +  # Adjust alpha to avoid overplotting, if necessary
  geom_smooth(method = "lm", color = "black", se = FALSE) +  # Add linear regression line without confidence band for clarity
  labs(title = "Linear Model of UNIT_SALES vs. DOLLAR_SALES by MANUFACTURER",
       x = "UNTI SALES",
       y = "DOLLAR SALES") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position if needed

```


> Behold the realm of DIET MOONLIT. Certain items sell much better, or wosrse with consideration of slop of dollars to units sold. While most of its realm is in the lower left hand portion, other brands have sales through both its unit and dollar sales vectors.

#Make the small casava df
> Investigating drinks with casava as a flavor in the Item description.

```{r}
# Create a new data frame with only the rows where the ITEM column contains the word 'casava'
casava_small <- df[grep("casava", df$ITEM, ignore.case = TRUE), ]

```

```{r}
skim(casava_small)
```

> Casava has a much lower unit sales and dollar sales at 71 and $184 than Diet Moonlight at 98 and $344. 

```{r}

# casava small is dataframe
#factor Region
casava_small$REGION <- as.factor(casava_small$REGION)
model <- lm(DOLLAR_SALES ~ UNIT_SALES + CALORIC_SEGMENT + PACKAGE + CATEGORY + SEASON + USPMKT, data = casava_small)
summary(model)


```

> Our Casava small has a lower R2 of 0.85, but also contains much more data with nearly 42K observations compared to our innovation df at about 5 observations. There are many signficant features, but nothing that swings the needle in huge ways.

## Cleaning

> Reworking the subset casava for more feature engineering.

```{r}

casava_small <- casava_small %>%
  mutate(
    PACKAGE2 = str_extract(ITEM, "(CUP|JUG).*"),  # Extracts the part from CUP or JUG to the end.
    ITEM = str_replace(ITEM, "(CUP|JUG).*", "")  # Replaces the CUP/JUG and everything after it with empty string in ITEM.
  )


#casava_small
```

```{r}


casava_small <- casava_small %>%
  mutate(
    TEMP = str_extract(ITEM, "\\d+\\.?\\d*.*"), # Extracts the part from the first number to the end.
    PACKAGE2 = if_else(is.na(PACKAGE2), TEMP, paste(PACKAGE2, TEMP)), # Combines existing PACKAGE2 with new extraction if needed.
    ITEM = str_replace(ITEM, "\\d+\\.?\\d*.*", ""), # Removes the numeric part and everything after it from ITEM.
    TEMP = NULL  # Removes the temporary column.
  )

#casava_small
```

```{r}
na_rows <- casava_small %>%
  filter(is.na(PACKAGE2))
#na_rows
#the above steps excised all packaging out of ITEM column
```

```{r}



casava_small <- casava_small %>%
  mutate(
    GENTLE_DRINK = if_else(str_detect(ITEM, "GENTLE DRINK"), 1, 0), # Assigns 1 if "GENTLE DRINK" exists, otherwise 0.
    ITEM = str_replace(ITEM, "GENTLE DRINK", "") # Removes "GENTLE DRINK" from ITEM.
  )
#casava_small
```

```{r}

casava_small <- casava_small %>%
  mutate(
    ENERGY_DRINK = if_else(str_detect(ITEM, "ENERGY DRINK"), 1, 0), # Assigns 1 if "ENERGY DRINK" exists, otherwise 0.
    ITEM = str_replace(ITEM, "ENERGY DRINK", "") # Removes "ENERGY DRINK" from ITEM.
  )

#casava_small
```


```{r}
library(dplyr)
library(stringr)

# Define the pattern as a regular expression
pattern <- "ZERO CALORIES|ZERO CALORIE|ZERO SUGAR|SUGAR FREE|NO CALORIES"

casava_small <- casava_small %>%
  mutate(
    CALORIC_SEGMENT_TEXT = str_extract(ITEM, pattern), # Extracts matching text based on the pattern.
    ITEM = str_replace_all(ITEM, pattern, "") # Removes extracted text from ITEM.
  )

#casava_small
```


```{r}
library(dplyr)
library(stringr)

casava_small <- casava_small %>%
  mutate(
    CALORIC_SEGMENT_TEXT = if_else(str_detect(ITEM, "\\bDIET\\b"), 
                                   if_else(is.na(CALORIC_SEGMENT_TEXT), "DIET", paste(CALORIC_SEGMENT_TEXT, "DIET", sep=", ")), 
                                   CALORIC_SEGMENT_TEXT)
  )
#casava_small
```

```{r}


# Function to remove the second instance of any repeating word
remove_second_instance <- function(item) {
  words <- unlist(str_split(item, "\\s+")) # Split item into words
  unique_words <- unique(words) # Get unique words to check for repeats
  for (word in unique_words) {
    word_indices <- which(words == word) # Find all indices of the current word
    if (length(word_indices) > 1) { # If there is more than one occurrence
      words[word_indices[2]] <- "" # Remove the second occurrence
    }
  }
  return(paste(words, collapse = " ")) # Reconstruct sentence without the second instance
}

# Apply the function to the 'ITEM' column
casava_small <- casava_small %>%
  mutate(ITEM = sapply(ITEM, remove_second_instance))


# Remove specific columns
casava_small <- select(casava_small, -PACKAGE2, -GENTLE_DRINK, -ENERGY_DRINK, -CALORIC_SEGMENT_TEXT)

```

```{r}
head(casava_small)
```



# FINAL THOUGHTS

DIET MOONLIT has pretty decent sales at 69th place in total revenue. Casava is not the sexiest flavor in town, but with our innovation dataframe the R2 is quite high (although it is based on regular and no specific package type). There are some weeks that look great for 6 month predictions, it's just a matter of deciding which ones to use.



> Data Prep and XgBoost Model for best weeks()



```{r}
df <- read_csv("swire_no_nas_w_pop.csv")  #inject the data and we will sub-sample

# #print unique package where BRAND is DIET MOONLIT
# unique(df$PACKAGE[df$BRAND == "DIET MOONLIT"])
# 
# #print count of CATEGORY = ENERGY with 2L in PACKAGE or ITEM
# table(df$CATEGORY[df$PACKAGE == "2L MULTI JUG" | df$ITEM == "2L MULTI JUG" | df$PACKAGE == "2L MULTI JUG" | df$ITEM == "2L MULTI JUG"])

#sales of BRAND=="MOONLIT"at Max date
df %>% 
  filter(BRAND == "MOONLIT") %>% 
    arrange(desc(DATE)) %>% 
  summarise(UNIT_SALES = sum(UNIT_SALES)) %>% 
  head(1)
  

df %>% 
  filter(BRAND == "MOONLIT") %>% 
  arrange((DATE)) %>% 
  group_by(DATE) %>%
  summarize(sum(UNIT_SALES)) %>% 
  head(1)

```
```{r}


#Group by ITEM with DATE Before 2021-01-01, drop those ITEM rows
df_long_running <- df %>%
  group_by(ITEM) %>%
  filter(DATE <= "2021-01-01")

#remove all rows in casava_long_running from casava
df <- df %>%
  anti_join(df_long_running)



#Group by ITEM rows with less than 20 weeks of data
df_small <- df %>%
  group_by(ITEM) %>%
  filter(n() <= 20)

#remove all rows in casava_long_running from casava
df <- df %>%
  anti_join(df_small)

#Drop rows after May 21st 2023 as there are several gaps for most brands in innovation casava
df <- df %>%
  filter(DATE <= "2023-05-21")

#cleanup everything but df
rm(df_long_running, df_small)

#skim(df)

```





```{r}
# Assuming df is your dataframe
# set.seed(123) # Set a random seed for reproducibility
# sampled_df <- df[sample(1:nrow(df), 2446143), ]
# rm(df)
```

```{r}
# df <- sampled_df
# rm(sampled_df)
```


```{r}
#skim(df)
```

```{r}
#summary(df)
```

```{r}
#casava CASAVA and MOONLIT (regular)
casava <- df %>%
#  filter(PACKAGE == "2L MULTI JUG",
         filter(
         str_detect(ITEM, "CASAVA"),
         BRAND == "MOONLIT")

#skim(casava)
```


```{r}
#DIET MOONLIT where package is either 2L MULTI JUG or 12SMALL MULTI CUP or 16SMALL 24ONE CUP
diet_moonlit <- df %>%
  filter(PACKAGE %in% c("2L MULTI JUG", "12SMALL", "16SMALL") &
           BRAND == "DIET MOONLIT")



#skim(diet_moonlit)
```

```{r}
#CASAVA and ENERGY
energy <- df %>% 
  filter(CATEGORY == "ENERGY",
         str_detect(ITEM, "CASAVA")
  )
```


```{r} 
# #Diet Energy - too much noise
diet_energy <- df %>% 
    filter(CATEGORY == "ENERGY",
            CALORIC_SEGMENT == 0)
```



```{r}
# Merge the data frames
merged_innovation_df <- bind_rows(casava, diet_moonlit, energy, diet_energy)
#merged_innovation_df <- bind_rows(casava, diet_moonlit, energy)
#remove duplicate rows
df <- merged_innovation_df %>% distinct()

skim(df)
```



```{r}

regions_joinme <- read.csv("states_summary.csv")

unique(regions_joinme$REGION)
# "NORTHERN"    "DESERT_SW"   "PRAIRIE"     "CALI_NEVADA"  "MOUNTAIN"    "SOCAL"   "ARIZONA"    "NEWMEXICO"   "NOCAL"    "COLORADO"    "KANSAS" 

str(regions_joinme)
# Perform a left join using the merge() function
df <- merge(df, regions_joinme[, c("MARKET_KEY", "REGION")], by = "MARKET_KEY", all.x = TRUE)
rm(regions_joinme)

```

```{r}
# Update CALORIC_SEGMENT values: 0 if 'DIET/LIGHT', otherwise 1
df$CALORIC_SEGMENT <- ifelse(df$CALORIC_SEGMENT == "DIET/LIGHT", 0, 1)
df$MARKET_KEY <- as.character(df$MARKET_KEY)
df <- df %>%
  mutate(
    MONTH = as.numeric(substr(DATE, 6, 7)),  # Extract the month from YYYY-MM-DD format
    SEASON = case_when(
      MONTH %in% c(12, 01, 02) ~ "WINTER",
      MONTH %in% c(03, 04, 05) ~ "SPRING",
      MONTH %in% c(06, 07, 08) ~ "SUMMER",
      MONTH %in% c(09, 10, 11) ~ "FALL",
      TRUE ~ NA_character_  # This is just in case there are any undefined values
    )
  )




```

# Cleaning Casava

```{r }
#save merged_innovation_df back to casava
casava <- df

skim(casava)



```

> Reworking the subset Casava for more feature engineering.

```{r}

casava <- casava %>%
  mutate(
    PACKAGE2 = str_extract(ITEM, "(CUP|JUG).*"),  # Extracts the part from CUP or JUG to the end.
    ITEM = str_replace(ITEM, "(CUP|JUG).*", "")  # Replaces the CUP/JUG and everything after it with empty string in ITEM.
  )



```

```{r}


casava <- casava %>%
  mutate(
    TEMP = str_extract(ITEM, "\\d+\\.?\\d*.*"), # Extracts the part from the first number to the end.
    PACKAGE2 = if_else(is.na(PACKAGE2), TEMP, paste(PACKAGE2, TEMP)), # Combines existing PACKAGE2 with new extraction if needed.
    ITEM = str_replace(ITEM, "\\d+\\.?\\d*.*", ""), # Removes the numeric part and everything after it from ITEM.
    TEMP = NULL  # Removes the temporary column.
  )

```

```{r}
na_rows <- casava %>%
  filter(is.na(PACKAGE2))
na_rows
#the above steps excised all packaging out of ITEM column
```


```{r}

# Function to remove the second instance of any repeating word
remove_second_instance <- function(item) {
  words <- unlist(str_split(item, "\\s+")) # Split item into words
  unique_words <- unique(words) # Get unique words to check for repeats
  for (word in unique_words) {
    word_indices <- which(words == word) # Find all indices of the current word
    if (length(word_indices) > 1) { # If there is more than one occurrence
      words[word_indices[2]] <- "" # Remove the second occurrence
    }
  }
  return(paste(words, collapse = " ")) # Reconstruct sentence without the second instance
}

# Apply the function to the 'ITEM' column
casava <- casava %>%
  mutate(ITEM = sapply(ITEM, remove_second_instance))

```

```{r }
# #One hot encode either "ENERGY" or "ED" in ITEM as an ENERGY_DRINK
# casava$ENERGY_DRINK <- ifelse(str_detect(casava$ITEM, "ENERGY|' ED'"), 1, 0)
# 
# casava$ITEM <- str_replace(casava$ITEM, "ENERGY DRINK", "")
# casava$ITEM <- str_replace(casava$ITEM, "ENERGY", "")
# casava$ITEM <- str_replace(casava$ITEM, " ED", "")
# table(casava$ENERGY_DRINK)
# 
# table(casava$CATEGORY)
# 
# casava %>% 
#   filter(ENERGY_DRINK == 1,
#          CATEGORY=='SSD') %>% 
#   select(ITEM) %>% 
#   head(10)

```

```{r}
# Remove specific columns
#casava <- select(casava, -PACKAGE2, -CATEGORY)

```

```{r}
head(casava)
table(casava$ITEM)
#write.csv(casava_small, "casava_yellow.csv", row.names = FALSE)
#write.csv(diet_moonlit_df, "diet_moonlit.csv", row.names = FALSE)
```
```{r}
#Trim trailing white space at end of ITEM
casava$ITEM <- str_trim(casava$ITEM, side = "right")

```

```{r}
# #replace "GENTLE DRINK" with "" in ITEM
# casava$ITEM <- str_replace(casava$ITEM, "GENTLE DRINK", "")

```


```{r }
#One hot encode "NO ARTIFICAL SWEETNERS" in ITEM
# casava$NO_ARTIFICIAL_SWEETNERS <- ifelse(str_detect(casava$ITEM, 
#                                                     "NO ARTIFICIAL SWEETENERS"), 
#                                          1, 0)
# 
# table(casava$NO_ARTIFICIAL_SWEETNERS)
```

```{r}
# # #Remove "NO ARTIFICIAL SWEETNERS" from ITEM
# casava$ITEM <- str_replace(casava$ITEM, "NO ARTIFICIAL SWEETENERS", "")
```

```{r}
#Remove Health Supplement rows
# casava <- casava %>%
#   filter(!str_detect(ITEM, "HEALTH SUPPLEMENT"))




```

```{r }

# pattern <- "ZERO CALORIES|ZERO CALORIE|ZERO SUGAR|SUGAR FREE|NO CALORIES|ZERO CARB|PURE ZERO|DIET"
# 
# casava <- casava %>%
#   mutate(
#     CALORIC_SEGMENT_TEXT = str_extract(ITEM, pattern), # Extracts matching text based on the pattern.
#     ITEM = str_replace_all(ITEM, pattern, "") # Removes extracted text from ITEM.
#   )

```

```{r }
#remove mythical beverage - monster reserve casava (pineapple)
#casava <- casava %>%
#  filter(!str_detect(ITEM, "MYTHICAL BEVERAGE"))

# Remove JUMPING JACK - sporadic single week
casava <- casava %>%
 filter(!str_detect(ITEM, "JUMPIN-FISH  CASAVA JACK"))

#Remove "SUPER-DUPER CASAVA YELLOW"
# casava <- casava %>%
#   filter(!str_detect(ITEM, "SUPER-DUPER PURE CASAVA YELLOW"))


#drop row with MOONLIT SUNSET in ITEM
casava <- casava %>%
  filter(!str_detect(ITEM, "MOONLIT  SUNSET"))

```


```{r}

print(unique(casava$ITEM))

# Find the minimum launch date for each product
min_launch_dates <- casava %>%
  group_by(ITEM) %>%
  summarise(min_launch_date = min(DATE))

# Join the minimum launch dates back to the original data
casava <- casava %>%
  left_join(min_launch_dates, by = "ITEM")

# Calculate the number of weeks since the product launch
casava <- casava %>%
  mutate(WEEKS_SINCE_LAUNCH = as.numeric(difftime(DATE, min_launch_date, units = "weeks")))

# Selecting required columns and printing the first 10 rows
casava %>%
  filter(UNIT_SALES > 0) %>%
  select(DATE, ITEM, WEEKS_SINCE_LAUNCH) %>%
  head(10)

#Subtract before April 19th from WEEKS_SINCE_LAUNCH where ITEM === "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA" actually launches and takes off, replace anything less than 0 with 0.

#Remove rows from ITEM == "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA" before April 19th 2021 when something about it actually launches
casava <- casava %>%
  filter(!(ITEM == "SUPER-DUPER JUICED ENERGY DRINK CASAVA SUNSET GUAVA" & DATE < as.Date("2021-04-19")))

#Subtract WEEKS SINCE LAUNCH for ITEM == "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA" by 16 weeks, so real launch date matches up.
casava <- casava %>%
  mutate(WEEKS_SINCE_LAUNCH = ifelse(ITEM == "SUPER-DUPER JUICED ENERGY DRINK CASAVA SUNSET GUAVA", WEEKS_SINCE_LAUNCH - 16, WEEKS_SINCE_LAUNCH))

#Set any negative WEEKS_SINCE_LAUNCH to 0
casava <- casava %>%
  mutate(WEEKS_SINCE_LAUNCH = ifelse(WEEKS_SINCE_LAUNCH < 0, 0, WEEKS_SINCE_LAUNCH))

#min date of ITEM "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA"
casava %>% 
  filter(ITEM == "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA") %>%
  summarise(min(DATE))



```




```{r}
print(unique(casava$ITEM))
print(unique(casava$BRAND))
print(unique(casava$CATEGORY))
print(unique(casava$PACKAGE))
#print(unique(casava$CALORIC_SEGMENT_TEXT))
print(unique(casava$CALORIC_SEGMENT))
```


```{r}
#What percent of UNIT SALE are "2L MULTI JUG"
casava %>%
  filter(PACKAGE == "2L MULTI JUG") %>%
  summarise(UNIT_SALES = sum(UNIT_SALES)) %>%
  mutate(PERCENTAGE = UNIT_SALES / sum(casava$UNIT_SALES) * 100)

#What percent of UNIT SALE are "DIET"
casava %>%
  filter(CALORIC_SEGMENT == "DIET") %>%
  summarise(UNIT_SALES = sum(UNIT_SALES)) %>%
  mutate(PERCENTAGE = UNIT_SALES / sum(casava$UNIT_SALES) * 100)

```


```{r }

#Test removing ITEMS containing MOONLIT CASAVA and MYSTICAL BEVERAGE
# unique(casava$BRAND)
# casava <- casava %>%
#   filter(!BRAND=="MOONLIT",
#          !BRAND=="MYTHICAL BEVERAGE")
# unique(casava$BRAND)
# 
 write_csv(casava, "casava_tableau.csv")

str(casava)

```



```{r}
#remove all objects other than casava
rm(list = setdiff(ls(), "casava"))

print(unique(casava$ITEM))
print(unique(casava$BRAND))
print(unique(casava$CATEGORY))
print(unique(casava$PACKAGE))

#what caloric segment is ITEM "MOONLIT SUNSET"
casava %>% 
  filter(ITEM == "MOONLIT SUNSET")
```



```{r}
# Creating an 'innovation' data frame

#factor all character variables
casava$REGION <- as.factor(casava$REGION)
#casava$CATEGORY <- as.factor(casava$CATEGORY)
#casava$BRAND <- as.factor(casava$BRAND)
casava$SEASON <- as.factor(casava$SEASON)
casava$PACKAGE2 <- as.factor(casava$PACKAGE2)



#model <- lm(DOLLAR_SALES ~ UNIT_SALES + CALORIC_SEGMENT + POP_SQMI + REGION + CATEGORY  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# model <- lm(DOLLAR_SALES ~ UNIT_SALES + POP_SQMI + REGION + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# summary(model)


```

```{r}
# Creating an 'innovation' data frame

#model <- lm(UNIT_SALES ~ DOLLAR_SALES + CALORIC_SEGMENT + PACKAGE + POP_SQMI + REGION + CATEGORY  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# model <- lm(UNIT_SALES ~ DOLLAR_SALES + PACKAGE + POP_SQMI + REGION  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# summary(model)

```


```{r}
# Creating an 'innovation' data frame

# #model <- lm(UNIT_SALES ~  CALORIC_SEGMENT + PACKAGE + POP_SQMI + REGION + CATEGORY  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# model <- lm(UNIT_SALES ~   + PACKAGE + POP_SQMI + REGION  + MONTH + SEASON + PACKAGE2 + WEEKS_SINCE_LAUNCH, data = casava)
# summary(model)

```







```{r}
# Load and prepare dataset
df <- read.csv("casava_tableau.csv") 
# Load and prepare dataset 

str(df)

casava <- df %>% 
  #select(-DATE, -MONTH, -SEASON, -BRAND, -REGION, -ITEM )
  #select(-MONTH, -SEASON, -min_launch_date, -PACKAGE2, -CALORIC_SEGMENT_TEXT)
  select(-MONTH, -SEASON, -min_launch_date, -PACKAGE2)

```

```{r}
# Assuming casava is your data frame and PACKAGING is the column of interest

# Create new columns in woodsy for each unique substring
# Each column will have a 1 if the substring is found in the PACKAGING column, 0 otherwise

casava$`16SMALL MULTI CUP` = as.integer(grepl("16SMALL MULTI CUP", casava$PACKAGE))
casava$`20SMALL MULTI JUG` = as.integer(grepl("20SMALL MULTI JUG", casava$PACKAGE))
casava$`16SMALL 24ONE CUP` = as.integer(grepl("16SMALL 24ONE CUP", casava$PACKAGE))
casava$`2L MULTI JUG` = as.integer(grepl("2L MULTI JUG", casava$PACKAGE))

     

#one hot encode non brand ITEM strings
casava$SUNSET = as.integer(grepl("SUNSET", casava$ITEM))
casava$BLAST = as.integer(grepl("BLAST", casava$ITEM))
casava$JUICED = as.integer(grepl("JUICED", casava$ITEM))
casava$GUAVA = as.integer(grepl("GUAVA", casava$ITEM))
casava$RECOVERY = as.integer(grepl("RECOVERY", casava$ITEM))
casava$JACK = as.integer(grepl("JACK", casava$ITEM))
casava$RESERVE= as.integer(grepl("RESERVE", casava$ITEM))
casava$WHITE= as.integer(grepl("WHITE", casava$ITEM))
casava$PITAYA= as.integer(grepl("PITAYA", casava$ITEM))
casava$ED= as.integer(grepl("ED", casava$ITEM))
casava$CASAVA= as.integer(grepl("CASAVA", casava$ITEM)) 

# Print the head of the data frame to see the first few rows
head(casava)

casava$CATEGORY <- NULL
casava$MARKET_KEY <- NULL
casava$MANUFACTURER <- NULL
casava$PACKAGE <- NULL


```
```{r}
library(fastDummies)

# One-hot encode the specified columns
casava <- fastDummies::dummy_cols(casava, select_columns = c("REGION", "ITEM"), remove_selected_columns = TRUE)
#casava <- fastDummies::dummy_cols(casava, select_columns = c("REGION", "SEASON","ITEM"), remove_selected_columns = TRUE)

# View the first few rows to verify the changes
head(casava)




write.csv(casava, "casava_one_hot.csv", row.names = FALSE)

```

```{r}

library(fastDummies)

# One-hot encode
casava <- fastDummies::dummy_cols(casava, select_columns = "BRAND", remove_selected_columns = TRUE)

# View the first few rows to verify
head(casava)

```


```{r}

#create new week of year column

casava <- casava %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(WEEK_OF_YEAR = lubridate::week(DATE))

#Drop DATE column
casava$DATE <- NULL

```





```{r}
# Summarize the dataset
skimr::skim(casava)
```

```{r}
#remove top one percent of unit sales to clean up outliers
df <- casava %>% 
  filter(UNIT_SALES < quantile(UNIT_SALES, 0.99))

```





```{r}

# Split the data
set.seed(123)
df_testtrn <- initial_split(df, prop = 0.8, strata = UNIT_SALES)
Train <- training(df_testtrn)
Test <- testing(df_testtrn)

# Prepare features and labels for XGBoost
train_features <- Train[, -which(names(Train) == "UNIT_SALES")]
train_labels <- Train$UNIT_SALES
test_features <- Test[, -which(names(Test) == "UNIT_SALES")]
test_labels <- Test$UNIT_SALES

# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(train_features), label = train_labels)
dtest <- xgb.DMatrix(data = as.matrix(test_features), label = test_labels)

```

```{r}



# Define XGBoost parameters
set.seed(123)
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.05,
  max_depth = 4,
  min_child_weight = 3,
  subsample = 0.7,
  colsample_bytree = 0.6,
  lambda = 1,
  alpha = 1
)

```

```{r}

# Perform cross-validation to find the optimal number of boosting rounds
cv_results <- xgb.cv(
  params = params,
  data = dtrain,  
  nfold = 5,
  nrounds = 500,  # Changed from 'num_boost_round' to 'nrounds'
  early_stopping_rounds = 10,
  metrics = "rmse",
  seed = 123
)
best_nrounds <- cv_results$best_iteration


```


```{r}
# Train the final model using the best number of rounds found
model_xgb <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)

```



```{r}

# Make predictions and evaluate the model
train_pred <- predict(model_xgb, dtrain)
test_pred <- predict(model_xgb, dtest)
train_rmse <- sqrt(mean((train_labels - train_pred)^2))
test_rmse <- sqrt(mean((test_labels - test_pred)^2))


```


```{r}

# Calculate R-squared for the training set
sst_train <- sum((train_labels - mean(train_labels)) ^ 2)
ssr_train <- sum((train_labels - train_pred) ^ 2)
r_squared_train <- 1 - (ssr_train / sst_train)

# Calculate R-squared for the test set
sst_test <- sum((test_labels - mean(test_labels)) ^ 2)
ssr_test <- sum((test_labels - test_pred) ^ 2)
r_squared_test <- 1 - (ssr_test / sst_test)

train_mape <- mean(abs((train_labels - train_pred) / train_labels)) * 100
test_mape <- mean(abs((test_labels - test_pred) / test_labels)) * 100
train_mae <- mean(abs(train_labels - train_pred))
test_mae <- mean(abs(test_labels - test_pred))



```


```{r}

cat("Model Performance Metrics:\n",
    "--------------------------\n",
    "Training RMSE: ", train_rmse, "\n",
    "Test RMSE: ", test_rmse, "\n",
    "Training R-squared: ", r_squared_train, "\n",
    "Test R-squared: ", r_squared_test, "\n",
    "Training MAE: ", train_mae, "\n",
    "Test MAE: ", test_mae, "\n",
    "Training MAPE: ", train_mape, "%\n",
    "Test MAPE: ", test_mape, "%\n", sep="")


```



```{r}
# Correcting Residuals Data Frame
# Assuming 'train_labels' and 'test_labels' contain the actual values,
# and 'train_pred' and 'test_pred' contain your model's predictions:

residuals_train <- train_labels - train_pred
residuals_test <- test_labels - test_pred

residuals_data <- data.frame(
  Residuals = c(residuals_train, residuals_test),
  Dataset = c(rep('Training', length(residuals_train)), rep('Test', length(residuals_test)))
)

# Now plotting residuals with corrected data
ggplot(residuals_data, aes(x = Residuals, fill = Dataset)) +
  geom_histogram(binwidth = 1, position = 'identity', alpha = 0.6) +
  facet_wrap(~ Dataset) +
  ggtitle('Residuals Distribution')


```



```{r}
# Assuming train_labels, test_labels, train_pred, and test_pred are correctly defined

# Adjusted Actual vs. Predicted Data Preparation
actual_pred_data <- data.frame(
  Actual = c(train_labels, test_labels),
  Predicted = c(train_pred, test_pred),
  Dataset = c(rep('Training', length(train_labels)), rep('Test', length(test_labels)))
)

# Plotting Actual vs. Predicted Values
ggplot(actual_pred_data, aes(x = Actual, y = Predicted, colour = Dataset)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed', color = 'red') +
  xlab('Actual Values') +
  ylab('Predicted Values') +
  scale_colour_manual(values = c('Training' = 'blue', 'Test' = 'red')) +
  ggtitle('Actual vs. Predicted Values')




```

```{r}


library(xgboost)

# Calculate feature importance
importance_matrix <- xgb.importance(feature_names = colnames(train_features), model = model_xgb)

# View the feature importance scores
print(importance_matrix)

```

```{r fig.width = 10, fig.height= 8}
# Plot the feature importance
xgb.plot.importance(importance_matrix = importance_matrix)

```

```{r fig.width=10, fig.height=8}
# Compute partial dependence data for 'DOLLAR_SALES' and 'CASAVA', CALORIC_SEGMENT, and "ENERGY
# pd <- partial(model_xgb, pred.var = c("DOLLAR_SALES", "CASAVA", "CALORIC_SEGMENT", ENERGY"), train = train_features, grid.resolution = 20)
# 
# # Default PDP
# pdp1 <- plotPartial(pd, plot = TRUE)
# 
# # Add contour lines and use a different color palette
# rwb <- colorRampPalette(c("red", "white", "blue"))
# pdp2 <- plotPartial(pd, contour = TRUE, col.regions = rwb)
# 
# # 3-D surface
# pdp3 <- plotPartial(pd, levelplot = FALSE, zlab = "Predicted Outcome", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
# 
# # Combine plots into one window
# grid.arrange(pdp1, pdp2, pdp3, ncol = 3)

```

## XGBOOST Model #2 
>Model with NO DOLLAR SALES Variable

```{r}
# Assuming 'df' is your complete dataframe and 'UNIT_SALES' is your target variable
df2 <- df
# Remove DOLLAR_SALES from the features
df2$DOLLAR_SALES <- NULL

# Split the updated data into training and testing sets (assuming you're using a similar approach as before)
set.seed(123)
df2_testtrn <- initial_split(df2, prop = 0.8, strata = UNIT_SALES)
Train <- training(df2_testtrn)
Test <- testing(df2_testtrn)

# Prepare features and labels for XGBoost, excluding DOLLAR_SALES
train_features <- Train[, -which(names(Train) == "UNIT_SALES")]
train_labels <- Train$UNIT_SALES
test_features <- Test[, -which(names(Test) == "UNIT_SALES")]
test_labels <- Test$UNIT_SALES

# Convert data to DMatrix format for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_features), label = train_labels)
dtest <- xgb.DMatrix(data = as.matrix(test_features), label = test_labels)

```

```{r}
# Assuming 'params' and 'best_nrounds' are defined as before

# Train the final model without DOLLAR_SALES
model_xgb_no_dollar_sales <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)

```

```{r}


# Make predictions and evaluate the model
train_pred <- predict(model_xgb_no_dollar_sales, dtrain)
test_pred <- predict(model_xgb_no_dollar_sales, dtest)
train_rmse <- sqrt(mean((train_labels - train_pred)^2))
test_rmse <- sqrt(mean((test_labels - test_pred)^2))


```


```{r}

# Calculate R-squared for the training set
sst_train <- sum((train_labels - mean(train_labels)) ^ 2)
ssr_train <- sum((train_labels - train_pred) ^ 2)
r_squared_train <- 1 - (ssr_train / sst_train)

# Calculate R-squared for the test set
sst_test <- sum((test_labels - mean(test_labels)) ^ 2)
ssr_test <- sum((test_labels - test_pred) ^ 2)
r_squared_test <- 1 - (ssr_test / sst_test)

train_mape <- mean(abs((train_labels - train_pred) / train_labels)) * 100
test_mape <- mean(abs((test_labels - test_pred) / test_labels)) * 100
train_mae <- mean(abs(train_labels - train_pred))
test_mae <- mean(abs(test_labels - test_pred))



```

```{r}
# Correcting Residuals Data Frame
# Assuming 'train_labels' and 'test_labels' contain the actual values,
# and 'train_pred' and 'test_pred' contain your model's predictions:

residuals_train <- train_labels - train_pred
residuals_test <- test_labels - test_pred

residuals_data <- data.frame(
  Residuals = c(residuals_train, residuals_test),
  Dataset = c(rep('Training', length(residuals_train)), rep('Test', length(residuals_test)))
)

# Now plotting residuals with corrected data
ggplot(residuals_data, aes(x = Residuals, fill = Dataset)) +
  geom_histogram(binwidth = 1, position = 'identity', alpha = 0.6) +
  facet_wrap(~ Dataset) +
  ggtitle('Residuals Distribution')


```

```{r}
# Assuming train_labels, test_labels, train_pred, and test_pred are correctly defined

# Adjusted Actual vs. Predicted Data Preparation
actual_pred_data <- data.frame(
  Actual = c(train_labels, test_labels),
  Predicted = c(train_pred, test_pred),
  Dataset = c(rep('Training', length(train_labels)), rep('Test', length(test_labels)))
)

# Plotting Actual vs. Predicted Values
ggplot(actual_pred_data, aes(x = Actual, y = Predicted, colour = Dataset)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = 'dashed', color = 'red') +
  xlab('Actual Values') +
  ylab('Predicted Values') +
  scale_colour_manual(values = c('Training' = 'blue', 'Test' = 'red')) +
  ggtitle('Actual vs. Predicted Values')




```

```{r}

cat("Model Performance Metrics:\n",
    "--------------------------\n",
    "Training RMSE: ", train_rmse, "\n",
    "Test RMSE: ", test_rmse, "\n",
    "Training R-squared: ", r_squared_train, "\n",
    "Test R-squared: ", r_squared_test, "\n",
    "Training MAE: ", train_mae, "\n",
    "Test MAE: ", test_mae, "\n",
    "Training MAPE: ", train_mape, "%\n",
    "Test MAPE: ", test_mape, "%\n", sep="")


```




```{r}

# Calculate feature importance
importance_matrix2 <- xgb.importance(feature_names = colnames(train_features), model = model_xgb_no_dollar_sales)

# View the feature importance scores
print(importance_matrix2)
```

```{r fig.width= 10, fig.height= 8}
xgb.plot.importance(importance_matrix = importance_matrix2)
```



```{r}
if (!requireNamespace("pdp", quietly = TRUE)) install.packages("pdp")
if (!requireNamespace("xgboost", quietly = TRUE)) install.packages("xgboost")
library(pdp)
library(xgboost)

```




```{r}
pdp::partial(model_xgb_no_dollar_sales, pred.var = "WEEK_OF_YEAR", train = train_features)

pd <- partial(model_xgb_no_dollar_sales, pred.var = "WEEK_OF_YEAR", train = train_features, grid.resolution = 20)

# Default PDP
pdp1 <- plotPartial(pd, plot = TRUE)



# plot
grid.arrange(pdp1)
```

> Based on the Casava Energy Drink innovation datafram we expect the best 6 months to be
between about weeks 20 and weeks 41. Form looking at Plum, we know that the dummy data predictions do not work well with the data and XGBoost.


```{r}
#cleanup all obj
rm(list = ls())
```

> Stochastic Weighted Average model for Casava Energy Drink, this dumps out the predicted demand. We then used Excel to calculate the optimal order quantity based on the predicted demand and the cost of overstocking and understocking using a simple NewsVendor Model. All visualtions were done in Tabelau.



```{r}
df <- read_csv("casava_tableau.csv")  #inject the df and we will sub-sample

```
```{r}
head(df)
```

```{r}
#filter package by "16SMALL MULTI CUP" and filter out non super-duper
df <- df %>%
  filter(PACKAGE == "16SMALL MULTI CUP",
        # str_detect(ITEM,"SUPER-DUPER|MYTHICAL"))
        str_detect(ITEM,"SUPER-DUPER"))

print(unique(df$ITEM))
```




```{r}
#SUM of Sales of "SUPER-DUPER JUICED  CASAVA SUNSET GUAVA", "SUPER-DUPER PITAYA  CASAVA", "SUPER-DUPER RECOVERY  CASAVA JACK" by WEEKS_SINCE_LAUNCH keep DATE column. Average super duper sales and take ratio of Diet Moonlit to Regular 
df <- df %>%
  group_by(WEEKS_SINCE_LAUNCH) %>%
  summarise(INNOVATION_UNIT_SALES = (sum(UNIT_SALES)) * 0.5/3) %>%
  ungroup()  # This ungroups the df after summarizing, making it easier to work with

# Now you can select all three columns: WEEKS_SINCE_LAUNCH, SUM_UNIT_SALES, and DATE
df <- df %>%
  select(WEEKS_SINCE_LAUNCH, INNOVATION_UNIT_SALES)
```


```{r}
# #Convert WEEKS_SINCE_LAUNCH to a DATE starting at week 20 of 2021
# df$DATE <- as.Date("2021-05-17") + (df$WEEKS_SINCE_LAUNCH - 1) * 7
# df <- df %>%
#   select(DATE, INNOVATION_UNIT_SALES)
# 
# head(df)


```


```{r}
#df

```

```{r}
write_csv(df,"cassava_newsvendor.csv")
```

